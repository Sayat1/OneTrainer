OUTNAME = "Chaewon-ADAM8-OT-12-FINETUNE-NOCAP-NOREG-EMA999-B5-lr5e6-10TE-MJ7-CFC-150E"
OUTPUTPATH = "/content/drive/MyDrive/Training/model/"+OUTNAME+".safetensors"

NAI=nai
#NAI="runwayml/stable-diffusion-v1-5"
#NAI="/content/drive/MyDrive/Training/exists/t3_Ver111_prune.safetensors"

CONCEPT="/content/my_concept.json"

TRAIN_METHOD="FINE_TUNE"

EMA="GPU" #OFF #GPU #CPU
EMA_DECAY="0.999"

UNET_LR="5e-06"
TE_LR="5e-07"
TRAIN_TE = True
TE_LAYER_SKIP=2

WEIGHT_DTYPE="FLOAT_32"
TRAIN_DTYPE="FLOAT_16"
OUT_DTYPE="FLOAT_16"

CLEAN_CACHE = True
BUCKET = True

RESOLUTION=624
EPOCH=150
SCHEDULER = "CONSTANT"
LR_WARMUP=640
LR_CYCLES=1
MAX_NOISE=1
GRADIENT_ACCUMULATION=1
BATCH_SIZE=5

LOSS_SCALER="BOTH" #GRADIENT_ACCUMULATION #BATCH #BOTH #NONE
LR_SCALER="BOTH" #GRADIENT_ACCUMULATION #BATCH #BOTH #NONE

#OPTIMIZER_SETTINGS={"optimizer":"ADAFACTOR" ,"weight_decay":0.0, "scale_parameter":False, "relative_step":False, "warmup_init":False}
#OPTIMIZER_SETTINGS={"optimizer":"PRODIGY" ,"beta1":0.9, "beta2":0.99, "weight_decay":0.1, "decouple":True, "use_bias_correction":False, "safeguard_warmup":False, "d0":1e-06, "d_coef":1.0, "growth_rate":"inf"}
#OPTIMIZER_SETTINGS={"optimizer":"DADAPT_ADAM", "beta1":0.9, "beta2":0.99, "weight_decay":0.1, "log_every":10, "decouple":True, "use_bias_correction":False, "d0":1e-08, "growth_rate":inf}
OPTIMIZER_SETTINGS={"optimizer":"ADAMW_8BIT", "beta1":0.9 ,"beta2":0.999 ,"weight_decay":0.0}
#OPTIMIZER_SETTINGS={"optimizer":"ADAMW", "beta1":0.9 ,"beta2":0.99 ,"weight_decay":0.1}

LORA_CONTINUE=""
LORA_RANK="128"
LORA_ALPHA="8"
LORA_CONV_RANK="128"
LORA_CONV_ALPHA="64"
#LORA_MODULES=["attentions"]
LORA_MODULES=["attentions","resnets","upsamplers","downsamplers"]

BACKUP_AFTER = "100"
#BACKUP_AFTER_UNIT = "EPOCH"
BACKUP_AFTER_UNIT = "NEVER"
ROLLING_BACKUP = True
ROLLING_BACKUP_COUNT = 1
BACKUP_BEFORE_SAVE = False

SAVE_AFTER = "50"
SAVE_AFTER_UNIT = "EPOCH"
ROLLING_SAVE_COUNT = 1